{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Please leave a ⭐ and fork the repository if you want.\n","\n","https://github.com/Ahus-AIM/Open-ECG-Digitizer\n","\n","https://arxiv.org/abs/2510.19590\n","\n","No segmentation model parameters have been adapted to the kaggle training set.\n","\n","No hyperparameters have been rigorously tuned on the kaggle training set (we just made sure it looked OK on folder no. 100642785)."]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-11-26T12:58:56.434210Z","iopub.status.busy":"2025-11-26T12:58:56.433694Z","iopub.status.idle":"2025-11-26T12:59:03.935155Z","shell.execute_reply":"2025-11-26T12:59:03.934594Z","shell.execute_reply.started":"2025-11-26T12:58:56.434191Z"},"trusted":true},"outputs":[{"ename":"TypeError","evalue":"unsupported operand type(s) for |: 'type' and '_GenericAlias'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32me:\\sjtu\\research\\experiment\\digital_twin_code\\ECG_digitization\\Open-ECG-Digitizer\\open-ecg-digitizer.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/sjtu/research/experiment/digital_twin_code/ECG_digitization/Open-ECG-Digitizer/open-ecg-digitizer.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39munet\u001b[39;00m \u001b[39mimport\u001b[39;00m UNet\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/sjtu/research/experiment/digital_twin_code/ECG_digitization/Open-ECG-Digitizer/open-ecg-digitizer.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mperspective_detector\u001b[39;00m \u001b[39mimport\u001b[39;00m PerspectiveDetector\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/sjtu/research/experiment/digital_twin_code/ECG_digitization/Open-ECG-Digitizer/open-ecg-digitizer.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcropper\u001b[39;00m \u001b[39mimport\u001b[39;00m Cropper\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/sjtu/research/experiment/digital_twin_code/ECG_digitization/Open-ECG-Digitizer/open-ecg-digitizer.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpixel_size_finder\u001b[39;00m \u001b[39mimport\u001b[39;00m PixelSizeFinder\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/sjtu/research/experiment/digital_twin_code/ECG_digitization/Open-ECG-Digitizer/open-ecg-digitizer.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msignal_extractor\u001b[39;00m \u001b[39mimport\u001b[39;00m SignalExtractor\n","File \u001b[1;32me:\\sjtu\\research\\experiment\\digital_twin_code\\ECG_digitization\\Open-ECG-Digitizer\\src\\model\\cropper.py:7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m perspective\n\u001b[1;32m----> 7\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCropper\u001b[39;00m(torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      8\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m      9\u001b[0m         \u001b[39mself\u001b[39m, granularity: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m, percentiles: Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39m0.01\u001b[39m, \u001b[39m0.99\u001b[39m), alpha: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m\n\u001b[0;32m     10\u001b[0m     ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m        The Cropper module is used to correct for perspective distortion in images, while also cropping the image to mostly include the signal.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m            alpha (float): A factor to control the influence of the source points on the destination points in the perspective transformation.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m        \"\"\"\u001b[39;00m\n","File \u001b[1;32me:\\sjtu\\research\\experiment\\digital_twin_code\\ECG_digitization\\Open-ECG-Digitizer\\src\\model\\cropper.py:252\u001b[0m, in \u001b[0;36mCropper\u001b[1;34m()\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39m    Solves for x in\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39m        torch.Tensor: The x-coordinates.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39my \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mtan(theta) \u001b[39m+\u001b[39m rho \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39mcos(theta)\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_perspective\u001b[39m(\n\u001b[1;32m--> 252\u001b[0m     \u001b[39mself\u001b[39m, input_tensor: torch\u001b[39m.\u001b[39mTensor, source_points: torch\u001b[39m.\u001b[39mTensor, fill_value: \u001b[39mfloat\u001b[39;49m \u001b[39m|\u001b[39;49m List[\u001b[39mfloat\u001b[39;49m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    253\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    254\u001b[0m     original_shape \u001b[39m=\u001b[39m input_tensor\u001b[39m.\u001b[39mshape\n\u001b[0;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m input_tensor\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:  \u001b[39m# (H, W)\u001b[39;00m\n","\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'type' and '_GenericAlias'"]}],"source":["import torch\n","import glob\n","import os.path\n","import yaml\n","import torch.nn.functional as F\n","import pandas as pd\n","import numpy as np\n","from torchvision.io import read_image\n","from torch import Tensor\n","# Imports from https://github.com/Ahus-AIM/Open-ECG-Digitizer\n","from src.model.unet import UNet\n","from src.model.perspective_detector import PerspectiveDetector\n","from src.model.cropper import Cropper\n","from src.model.pixel_size_finder import PixelSizeFinder\n","from src.model.signal_extractor import SignalExtractor\n","from src.model.lead_identifier import LeadIdentifier\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameters - please optimize these using the training set!\n","\n","Some descriptions of the modules: https://arxiv.org/abs/2510.19590"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-11-26T12:59:03.936560Z","iopub.status.busy":"2025-11-26T12:59:03.936180Z","iopub.status.idle":"2025-11-26T12:59:03.943034Z","shell.execute_reply":"2025-11-26T12:59:03.942140Z","shell.execute_reply.started":"2025-11-26T12:59:03.936541Z"},"trusted":true},"outputs":[],"source":["# Resample size: trade-off between accuracy and speed/vram\n","resample_size = 3000 # maximum no. pixels along the longest dimension\n","\n","# PixelSizeFinder\n","PixelSizeFinderKwargs = dict(\n","    mm_between_grid_lines=5,\n","    samples=1000,\n","    min_number_of_grid_lines=20,\n","    max_number_of_grid_lines=100,\n","    max_zoom=10,\n","    zoom_factor=10.0,\n","    lower_grid_line_factor=0.1,\n",")\n","\n","# SignalExtractor\n","SignalExtractorKwargs = dict(\n","    threshold_sum=10.0,\n","    threshold_line_in_mask=0.8,\n","    label_thresh=0.05,\n","    max_iterations=4,\n","    split_num_stripes=4,\n","    candidate_span=10,\n","    lam=0.5,\n","    min_line_width=30,\n",")\n","\n","# PerspectiveDetector\n","PerspectiveDetectorKwargs = dict(\n","    num_thetas=200, # Higher -> more accurate but slower and more VRAM\n","    max_num_nonzero=10_000,\n",")\n","\n","# LeadIdentifier\n","LeadIdentifierKwargs = dict(\n","    layouts=None,  # should be a dict[str, Any]\n","    unet=None,     # should be a torch.nn.Module\n","    device=None,   # should be a torch.device\n","    possibly_flipped=True,\n","    target_num_samples=None,\n","    required_valid_samples=2,\n","    debug=False,\n",")\n","\n","# Cropper\n","CropperKwargs = dict(\n","    granularity=50,\n","    percentiles=(0.03, 0.97),\n","    alpha=0.95,\n",")\n","\n","# Layout UNet (you CAN change this but then you need to retrain the net)\n","# https://github.com/Ahus-AIM/Open-ECG-Digitizer/blob/main/src/train.py\n","# https://github.com/Ahus-AIM/Open-ECG-Digitizer/blob/main/src/config/lead_name_unet.yml\n","LayoutUNetKwargs = dict(\n","    weights_path=\"/kaggle/input/open-ecg-digitizer-weights/pytorch/default/1/lead_name_unet_weights_07072025.pt\",\n","    num_in_channels=1,\n","    num_out_channels=13,\n","    dims=[32, 64, 128, 256, 256],\n","    depth=2,\n",")\n","\n","# Segmentation UNet (you CAN change this but then you need to retrain the net)\n","# https://github.com/Ahus-AIM/Open-ECG-Digitizer/blob/main/src/train.py\n","# https://github.com/Ahus-AIM/Open-ECG-Digitizer/blob/main/src/config/unet.yml\n","LoadModelKwargs = dict(\n","    weights_path=\"/kaggle/input/open-ecg-digitizer-weights/pytorch/default/1/unet_weights_07072025.pt\",\n","    num_in_channels=3,\n","    num_out_channels=4,\n","    dims=[32, 64, 128, 256, 320, 320, 320, 320],\n","    depth=2,\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Utility code for plotting and orchestrating the modules\n","\n","You can also use https://github.com/Ahus-AIM/Open-ECG-Digitizer/blob/main/src/model/inference_wrapper.py for orchestrating the modules if you want."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-11-26T13:11:44.753506Z","iopub.status.busy":"2025-11-26T13:11:44.752885Z","iopub.status.idle":"2025-11-26T13:11:45.204248Z","shell.execute_reply":"2025-11-26T13:11:45.203494Z","shell.execute_reply.started":"2025-11-26T13:11:44.753483Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def add_noise_to_image(input_img, sigma=1.0, opacity=0.85):\n","    noise = torch.sigmoid(torch.randn_like(input_img) * sigma)\n","    input_img = (opacity)*input_img + (1-opacity) * noise\n","    return input_img\n","\n","def load_model(**kwargs):\n","    weights_path = kwargs.pop(\"weights_path\", None)  # safely extract\n","    model = UNet(**kwargs)\n","    state_dict = torch.load(weights_path, map_location=device)\n","    state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n","    model.load_state_dict(state_dict)\n","    model.eval().to(device)\n","    return model\n","\n","\n","def load_png_file(path):\n","    img = read_image(path)\n","    img = img.float() / 255.0\n","    img = img.unsqueeze(0)\n","    if img.shape[1] > 3:\n","        img = img[:, :3, :, :]\n","    if img.shape[2]>img.shape[3]:\n","        img = img.transpose(2, 3).flip(3) \n","    return img\n","\n","\n","def _crop_y(\n","    image: Tensor,\n","    signal_prob: Tensor,\n","    grid_prob: Tensor,\n","    text_prob: Tensor,\n",") -> tuple[Tensor, Tensor, Tensor, Tensor]:\n","    def get_bounds(tensor: Tensor) -> tuple[int, int]:\n","        prob = torch.clamp(\n","            tensor.squeeze().sum(dim=tensor.dim() - 3)\n","            - tensor.squeeze().sum(dim=tensor.dim() - 3).mean(),\n","            min=0,\n","        )\n","        non_zero = (prob > 0).nonzero(as_tuple=True)[0]\n","        if non_zero.numel() == 0:\n","            return 0, tensor.shape[2] - 1\n","        return int(non_zero[0].item()), int(non_zero[-1].item())\n","\n","    y1, y2 = get_bounds(signal_prob + grid_prob)\n","\n","    slices = (slice(None), slice(None), slice(y1, y2 + 1), slice(None))\n","    return (\n","        image[slices],\n","        signal_prob[slices],\n","        grid_prob[slices],\n","        text_prob[slices],\n","    )\n","\n","\n","def _align_feature_maps(\n","    cropper: Cropper,\n","    image: Tensor,\n","    signal_prob: Tensor,\n","    grid_prob: Tensor,\n","    text_prob: Tensor,\n","    source_points: Tensor,\n",") -> tuple[Tensor, Tensor, Tensor, Tensor]:\n","    aligned_signal_prob = cropper.apply_perspective(\n","        signal_prob,\n","        source_points,\n","        fill_value=0,\n","    )\n","    aligned_image = cropper.apply_perspective(\n","        image,\n","        source_points,\n","        fill_value=0,\n","    )\n","    aligned_grid_prob = cropper.apply_perspective(\n","        grid_prob,\n","        source_points,\n","        fill_value=0,\n","    )\n","    aligned_text_prob = cropper.apply_perspective(\n","        text_prob,\n","        source_points,\n","        fill_value=0,\n","    )\n","    (\n","        aligned_image,\n","        aligned_signal_prob,\n","        aligned_grid_prob,\n","        aligned_text_prob,\n","    ) = _crop_y(\n","        aligned_image,\n","        aligned_signal_prob,\n","        aligned_grid_prob,\n","        aligned_text_prob,\n","    )\n","\n","    return (\n","        aligned_image,\n","        aligned_signal_prob,\n","        aligned_grid_prob,\n","        aligned_text_prob,\n","    )\n","\n","\n","def plot_segmentation_and_image(\n","    image,\n","    segmentation,\n","    aligned_signal,\n","    aligned_grid,\n","    lines,\n","):\n","    import matplotlib.pyplot as plt\n","    import numpy as np\n","    import torch\n","    import os\n","\n","    output_dir = '/kaggle/working'\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # -----------------------------\n","    # 1. 原始图像\n","    # -----------------------------\n","    image_np = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n","\n","    plt.figure(figsize=(6,6))\n","    plt.imshow(image_np)\n","    plt.axis(\"off\")\n","    plt.tight_layout()\n","    plt.savefig(output_dir+\"/1_raw_image.png\", dpi=300)\n","    plt.close()\n","\n","    # -----------------------------\n","    # 2. segmentation feature map\n","    # -----------------------------\n","    probs = segmentation.squeeze(0).cpu()\n","    show_featuremap = torch.ones(probs.shape[1], probs.shape[2], 3)\n","    probs[2] /= probs[2].max()\n","    show_featuremap[:, :, [0, 1, 2]] -= 2 * probs[2].unsqueeze(-1)\n","    show_featuremap[:, :, [1, 2]] -= probs[0].unsqueeze(-1)\n","    show_featuremap = torch.clamp(show_featuremap, 0, 1).numpy()\n","\n","    plt.figure(figsize=(6,6))\n","    plt.imshow(show_featuremap)\n","    plt.axis(\"off\")\n","    plt.tight_layout()\n","    plt.savefig(output_dir+\"/2_segmentation_featuremap.png\", dpi=300)\n","    plt.close()\n","\n","    # -----------------------------\n","    # 3. straightened image\n","    # -----------------------------\n","    straightened_featuremap = torch.ones(\n","        aligned_signal.shape[2],\n","        aligned_signal.shape[3],\n","        3,\n","        device=aligned_signal.device,\n","    )\n","    aligned_signal /= aligned_signal.max()\n","    straightened_featuremap[:, :, [0, 1, 2]] -= 2 * aligned_signal[0, 0].unsqueeze(-1)\n","    aligned_grid /= aligned_grid.max()\n","    straightened_featuremap[:, :, [1, 2]] -= aligned_grid[0, 0].unsqueeze(-1)\n","    straightened_featuremap = torch.clamp(straightened_featuremap, 0, 1)\n","\n","    plt.figure(figsize=(6,6))\n","    plt.imshow(straightened_featuremap.cpu())\n","    plt.axis(\"off\")\n","    plt.tight_layout()\n","    plt.savefig(output_dir+\"/3_straightened_featuremap.png\", dpi=300)\n","    plt.close()\n","\n","    # -----------------------------\n","    # 4. lines 图\n","    # -----------------------------\n","    plt.figure(figsize=(6,6))\n","    if lines.numel() > 0:\n","        offsets = [-0, -10.5, -7, -0, -3.5, -7, -0, -3.5, -7, -0, -3.5, -7]\n","        plt.plot(lines.T.cpu().numpy() + offsets[: lines.shape[0]])\n","        plt.plot(lines[1,:2500].T.cpu().numpy() -3.5)\n","    plt.axis(\"off\")\n","    plt.tight_layout()\n","    plt.savefig(output_dir+\"/4_lines.png\", dpi=300)\n","    plt.close()\n","\n","    # -----------------------------\n","    # 5. 保存四宫格大图（原始你的版本）\n","    # -----------------------------\n","    fig, ax = plt.subplots(2, 2, figsize=(16, 12))\n","    ax[0, 0].imshow(image_np)\n","    ax[0, 0].axis(\"off\")\n","\n","    ax[0, 1].imshow(show_featuremap)\n","    ax[0, 1].axis(\"off\")\n","\n","    ax[1, 0].imshow(straightened_featuremap.cpu())\n","    ax[1, 0].axis(\"off\")\n","\n","    if lines.numel() > 0:\n","        ax[1, 1].plot(lines.T.cpu().numpy() + offsets[: lines.shape[0]])\n","        ax[1, 1].plot(lines[1,:2500].T.cpu().numpy() -3.5)\n","    ax[1, 1].axis(\"off\")\n","\n","    plt.tight_layout()\n","    fig.savefig(output_dir+\"/0_full_grid.png\", dpi=200)\n","    plt.close(fig)\n","\n","\n","\n","def crop_image(image, probs):\n","    perspective_detector = PerspectiveDetector(**PerspectiveDetectorKwargs)\n","\n","    cropper = Cropper(**CropperKwargs)\n","\n","    alignment_params = perspective_detector(probs[0, 0])\n","\n","    source_points = cropper(probs[0, 1], alignment_params)\n","\n","    signal_prob, grid_prob, text_prob = (\n","        probs[:, [2]],\n","        probs[:, [0]],\n","        probs[:, [1]],\n","    )\n","\n","    (\n","        aligned_image,\n","        aligned_signal_prob,\n","        aligned_grid_prob,\n","        aligned_text_prob,\n","    ) = _align_feature_maps(\n","        cropper,\n","        image,\n","        signal_prob,\n","        grid_prob,\n","        text_prob,\n","        source_points,\n","    )\n","\n","    return (\n","        aligned_image,\n","        aligned_signal_prob,\n","        aligned_grid_prob,\n","        aligned_text_prob,\n","    )\n","\n","\n","def extract_signals(\n","    aligned_signal_prob: Tensor,\n","    aligned_grid_prob: Tensor,\n","    aligned_text_prob: Tensor,\n","    target_num_samples: int,\n",") -> Tensor:\n","    pixel_size_finder = PixelSizeFinder(**PixelSizeFinderKwargs)\n","    signal_extractor = SignalExtractor(**SignalExtractorKwargs)\n","\n","    layout_unet = load_model(\n","        **LayoutUNetKwargs,\n","    )\n","\n","    layouts = yaml.safe_load(\n","        open(\"src/config/lead_layouts_george-moody-2024.yml\", \"r\"),\n","    )\n","\n","    identifier_kwargs = LeadIdentifierKwargs.copy()\n","    identifier_kwargs.update(\n","        dict(\n","            layouts=layouts,\n","            unet=layout_unet,\n","            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n","            target_num_samples=target_num_samples,\n","        )\n","    )\n","    identifier = LeadIdentifier(**identifier_kwargs)\n","    mm_per_pixel_x, mm_per_pixel_y = pixel_size_finder(aligned_grid_prob)\n","\n","    avg_pixel_per_mm = (1 / mm_per_pixel_x + 1 / mm_per_pixel_y) / 2 # Is there a better way?\n","    print('prob',aligned_signal_prob.shape)\n","    signals = signal_extractor(aligned_signal_prob.squeeze())\n","    \n","    signals = identifier(\n","        signals,\n","        aligned_text_prob,\n","        avg_pixel_per_mm=avg_pixel_per_mm,\n","    )\n","\n","    return signals\n","\n","def resample_image(image: Tensor, resample_size: int) -> Tensor:\n","    height, width = image.shape[2], image.shape[3]\n","    min_dim = min(height, width)\n","    max_dim = max(height, width)\n","\n","    if isinstance(resample_size, int):\n","        if max_dim > resample_size:\n","            scale = resample_size / max_dim\n","            new_size = (int(height * scale), int(width * scale))\n","            return F.interpolate(image, size=new_size, mode=\"bilinear\", align_corners=False, antialias=True)\n","        return image\n","\n","    if isinstance(resample_size, tuple):\n","        interpolated = F.interpolate(\n","            image, size=resample_size, mode=\"bilinear\", align_corners=False, antialias=True\n","        )\n","        return interpolated\n","\n","    raise ValueError(f\"Invalid resample_size: {resample_size}. Expected int or tuple of (height, width).\")\n","\n","leads_names = ['I','II','III','aVR','aVL','aVF','V1','V2','V3','V4','V5','V6']\n","def get_slice(lead_name: str, number_of_rows: int) -> slice:\n","    assert lead_name in leads_names\n","    if lead_name in (\"II\",):\n","        return slice(0, number_of_rows)\n","    if lead_name in ((\"I\", \"III\")):\n","        return slice(0, number_of_rows)\n","    if lead_name in ((\"aVR\", \"aVF\", \"aVL\")):\n","        return slice(1*number_of_rows, 2*number_of_rows)\n","    if lead_name in ((\"V1\", \"V2\", \"V3\")):\n","        return slice(2*number_of_rows, 3*number_of_rows)\n","    if lead_name in ((\"V4\", \"V5\", \"V6\")):\n","        return slice(3*number_of_rows, 4*number_of_rows)\n","\n","def digitize_image(input_img: Tensor, resample_size: int, target_num_samples: int) -> Tensor:\n","    # input_img = add_noise_to_image(input_img) # The UNet is trained for \"real\" images. Sometimes it performs better with added noise for generated images.\n","    input_img = resample_image(image=input_img, resample_size=resample_size) # higher resample size is (probably) better but watch out for VRAM and time consraints\n","    \n","    with torch.no_grad():\n","        logits = model(input_img.to(device))\n","        output_probs = torch.softmax(logits, dim=1)\n","        print()\n","        aligned_image, aligned_signal, aligned_grid, aligned_text = crop_image(input_img, output_probs)\n","        lines = extract_signals(aligned_signal, aligned_grid, aligned_text, target_num_samples=target_num_samples)\n","        lines = lines[\"canonical_lines\"] * 1e-3  # microvolt to millivolt\n","\n","    return output_probs, aligned_signal, aligned_grid, lines.float()\n","    \n","\n","model = load_model(**LoadModelKwargs)"]},{"cell_type":"markdown","metadata":{},"source":["# Visualize a few example images."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-11-26T13:11:47.696556Z","iopub.status.busy":"2025-11-26T13:11:47.696210Z","iopub.status.idle":"2025-11-26T13:12:31.923851Z","shell.execute_reply":"2025-11-26T13:12:31.923113Z","shell.execute_reply.started":"2025-11-26T13:11:47.696534Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Segmenting /kaggle/input/mancy-ecg-dataset/NUH_yilin.jpg...\n","\n","prob torch.Size([1, 1, 1747, 3200])\n","torch.Size([12, 10000])\n","slice(0, 2500, None)\n","(2500,)\n","slice(0, 10000, None)\n","(10000,)\n","slice(0, 2500, None)\n","(2500,)\n","slice(2500, 5000, None)\n","(2500,)\n","slice(2500, 5000, None)\n","(2500,)\n","slice(2500, 5000, None)\n","(2500,)\n","slice(5000, 7500, None)\n","(2500,)\n","slice(5000, 7500, None)\n","(2500,)\n","slice(5000, 7500, None)\n","(2500,)\n","slice(7500, 10000, None)\n","(2500,)\n","slice(7500, 10000, None)\n","(2500,)\n","slice(7500, 10000, None)\n","(2500,)\n"]}],"source":["# paths = [\n","#     # \"/kaggle/input/physionet-ecg-image-digitization/train/1006427285/1006427285-0004.png\",\n","#     \"/kaggle/input/physionet-ecg-image-digitization/train/1006427285/1006427285-0005.png\",\n","#     # \"/kaggle/input/physionet-ecg-image-digitization/train/1006427285/1006427285-0006.png\",\n","# ]\n","paths = ['/kaggle/input/mancy-ecg-dataset/NUH_yilin.jpg']\n","leads_names =    ['I','II','III','aVR','aVL','aVF','V1','V2','V3','V4','V5','V6']\n","number_of_rows = [2500,10000,2500,2500,2500,2500,2500,2500,2500,2500,2500,2500]\n","output_path = \"/kaggle/working/\"\n","for path in paths:\n","    input_img = load_png_file(path)\n","    if not os.path.exists(path):\n","        continue\n","\n","    ### SIGNAL EXTRACTION ###\n","    print(f\"Segmenting {path}...\")\n","    \n","    output_probs, aligned_signal, aligned_grid, lines = digitize_image(input_img, 3200, 10000)\n","    print(lines.shape)\n","    plot_segmentation_and_image(input_img, output_probs, aligned_signal, aligned_grid, lines)\n","\n","    ### SAVING ###\n","    # save to csv\n","    # 假设总共有 10000 个时间点\n","    n_rows = 10000\n","    df = pd.DataFrame(index=range(n_rows))\n","    \n","    for i,lead_name in enumerate(leads_names):\n","        lead_data = lines[i]\n","        lead_slice = get_slice(lead_name, number_of_rows[i])\n","        print(lead_slice)\n","        # 将 Tensor 转成 numpy\n","        lead_values = lead_data[lead_slice].squeeze().cpu().numpy()\n","        print(lead_values.shape)\n","\n","        # 如果是 slice 类型：\n","        rows = df.index[lead_slice]  # 这样会得到一个长度严格匹配的 Index 对象\n","        df.loc[rows, lead_name] = lead_values\n","\n","    csv_path = os.path.join(output_path, os.path.basename(path).replace(\".png\", \".csv\").replace(\".jpg\", \".csv\"))\n","    df.to_csv(csv_path, index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Run the digitization"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-11-26T12:59:35.494481Z","iopub.status.busy":"2025-11-26T12:59:35.494187Z","iopub.status.idle":"2025-11-26T13:00:00.367612Z","shell.execute_reply":"2025-11-26T13:00:00.367035Z","shell.execute_reply.started":"2025-11-26T12:59:35.494457Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Segmenting /kaggle/input/physionet-ecg-image-digitization/test/1053922973.png...\n","\n","prob torch.Size([1, 1, 1532, 2200])\n","Segmenting /kaggle/input/physionet-ecg-image-digitization/test/2352854581.png...\n","\n","prob torch.Size([1, 1, 1613, 2200])\n"]}],"source":["test = pd.read_csv('/kaggle/input/physionet-ecg-image-digitization/test.csv')\n","output_path = '/kaggle/working/submission.csv'\n","\n","# Prepare the file (write header once)\n","if os.path.exists(output_path):\n","    os.remove(output_path)\n","pd.DataFrame(columns=[\"id\", \"value\"]).to_csv(output_path, index=False)\n","\n","old_id = None\n","\n","for index, row in test.iterrows():\n","    if row.id != old_id:\n","        old_id = row.id\n","    \n","        path = f\"/kaggle/input/physionet-ecg-image-digitization/test/{row.id}.png\"\n","        target_num_samples = row.fs * 10 # We assume 10 second signals, not optimal.\n","        input_img = load_png_file(path)\n","\n","        ### SIGNAL EXTRACTION ###\n","        print(f\"Segmenting {path}...\")\n","        output_probs, aligned_signal, aligned_grid, lines = digitize_image(input_img, resample_size, target_num_samples)\n","\n","        ### (optional) VISUALIZATION ###\n","        if False:\n","            plot_segmentation_and_image(input_img, output_probs, aligned_signal, aligned_grid, lines)\n","\n","    ### SAVING ###\n","    file_id = row.id\n","    lead_name = row.lead\n","    number_of_rows_in_lead = row.number_of_rows\n","\n","    index = leads_names.index(lead_name)\n","\n","    lead_data = lines[index]\n","    lead_data = lead_data[get_slice(lead_name, number_of_rows_in_lead)]\n","    \n","    mean_val = np.nanmean(lead_data)\n","    if np.isnan(mean_val):\n","        mean_val = 0.0\n","    lead_data = np.nan_to_num(lead_data, nan=mean_val)\n","\n","    assert len(lead_data) == number_of_rows_in_lead\n","    \n","    chunk = []\n","    for t in range(number_of_rows_in_lead):\n","        chunk.append({\"id\": f\"{file_id}_{t}_{lead_name}\", \"value\": float(lead_data[t])})\n","\n","    if chunk:\n","        pd.DataFrame(chunk).to_csv(output_path, mode='a', index=False, header=False)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":14096757,"sourceId":97984,"sourceType":"competition"},{"datasetId":8838912,"sourceId":13874419,"sourceType":"datasetVersion"},{"modelId":484689,"modelInstanceId":468838,"sourceId":623160,"sourceType":"modelInstanceVersion"},{"modelId":484698,"modelInstanceId":468846,"sourceId":623170,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":31154,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
